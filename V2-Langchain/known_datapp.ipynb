{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# import datasets\n",
    "\n",
    "#known unknown dataset\n",
    "known_fp = 'BIG-bench/bigbench/benchmark_tasks/known_unknowns/task.json'\n",
    "with open(known_fp, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'How often did Abraham Lincoln cut his toenails?', 'target_scores': {'Every Saturday night': 0, 'Unknown': 1}}\n",
      "{'input': 'When was Abraham Lincoln born?', 'target_scores': {'February 12, 1809': 1, 'Unknown': 0}}\n",
      "{'input': 'What did Barack Obama eat for breakfast yesterday?', 'target_scores': {'Eggs, potatoes, and toast': 0, 'Unknown': 1}}\n",
      "{'input': 'What does Barack Obama like to eat for breakfast?', 'target_scores': {'Eggs, potatoes, and toast': 1, 'Unknown': 0}}\n",
      "{'input': 'Who was the most recent person in the world to break their toe?', 'target_scores': {'Ariana Grande': 0, 'Unknown': 1}}\n"
     ]
    }
   ],
   "source": [
    "for q in data[\"examples\"][:5]:\n",
    "    print(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How often did Abraham Lincoln cut his toenails?\n",
      "{'Every Saturday night': 0, 'Unknown': 1}\n",
      "When was Abraham Lincoln born?\n",
      "{'February 12, 1809': 1, 'Unknown': 0}\n",
      "What did Barack Obama eat for breakfast yesterday?\n",
      "{'Eggs, potatoes, and toast': 0, 'Unknown': 1}\n",
      "What does Barack Obama like to eat for breakfast?\n",
      "{'Eggs, potatoes, and toast': 1, 'Unknown': 0}\n",
      "Who was the most recent person in the world to break their toe?\n",
      "{'Ariana Grande': 0, 'Unknown': 1}\n"
     ]
    }
   ],
   "source": [
    "for q in data[\"examples\"][:5]:\n",
    "    input_text = q[\"input\"]\n",
    "    target_text = q[\"target_scores\"]\n",
    "    print(input_text)\n",
    "    print(target_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Feature Engineering: Depending on your machine learning task, you may need to create new features or transform existing ones. Feature engineering can involve:\n",
    "\n",
    "Text preprocessing(tokenization, stemming, etc.) if the \"input\" contains textual data.\n",
    "Encoding categorical variables(e.g., one-hot encoding or label encoding).\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How often did Abraham Lincoln cut his toenails?\n",
      "When was Abraham Lincoln born?\n",
      "What did Barack Obama eat for breakfast yesterday?\n",
      "What does Barack Obama like to eat for breakfast?\n",
      "Who was the most recent person in the world to break their toe?\n"
     ]
    }
   ],
   "source": [
    "for q in data[\"examples\"][:5]:\n",
    "    input_text = q[\"input\"]\n",
    "    print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tevykuch/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOKENIZATION \n",
    "\n",
    "#input text\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "# Download punkt pre-trained dataset for nlp tasks (word seg, text split)\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# preprocessing pipeline: removing stopwords, stemming, or applying custom text-based transformations\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return ' '.join([stemmer.stem(word) for word in word_tokens])\n",
    "\n",
    "# Create a preprocessing pipeline\n",
    "text_preprocessor = Pipeline([\n",
    "    ('stemming', FunctionTransformer(func=stem_text, validate=False))\n",
    "])\n",
    "\n",
    "# Contain all the semantic entries for input data \n",
    "preprocessed_list = []\n",
    "truth_list = []\n",
    "\n",
    "\n",
    "# word tokenization \n",
    "for q in data[\"examples\"]:\n",
    "    input_text = q[\"input\"]\n",
    "    target_text = q[\"target_scores\"]\n",
    "    word_tokens = word_tokenize(input_text)\n",
    "\n",
    "    # Apply the preprocessing pipeline to  tokenized text data. Contains the result of applying text-based transformation after stemming is applied to each token \n",
    "    preprocessed_text = text_preprocessor.fit_transform(input_text)\n",
    "\n",
    "    #append the tokens of preprocessed text to the lists\n",
    "    preprocessed_list.append(preprocessed_text)\n",
    "    truth_list.append(target_text)\n",
    "\n",
    "\n",
    "# generate preprocessed dataset for train \n",
    "len(preprocessed_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (2782949705.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [168]\u001b[0;36m\u001b[0m\n\u001b[0;31m    word2vec_model = Word2Vec(sentences = preprocessed_list, vector_size=vector_size, window=window_size, min_count=min_count, sg=sg))\u001b[0m\n\u001b[0m                                                                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "#text vectors\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vector_size = 100  # Dimensionality of word embeddings\n",
    "window_size = 5    # Context window size\n",
    "min_count = 1      # Minimum word frequency\n",
    "sg = 0              # Use Skip-gram model (0 for CBOW)\n",
    "\n",
    "#create a word2vec model\n",
    "word2vec_model = Word2Vec(sentences = preprocessed_list, vector_size=vector_size, window=window_size, min_count=min_count, sg=sg))\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "word2vec_model.save('word2vec_model.bin')\n",
    "\n",
    "# Use the model to generate word embeddings\n",
    "#word_vector = word2vec_model.wv['example']  # Retrieve the embedding for the word 'example'\n",
    "\n",
    "#transform tokenized text into word vectors using word2vector model \n",
    "#find the mean for each word in each row sentence[0]\n",
    "# Transform tokenized text into word vectors using the Word2Vec model\n",
    "text_vectors = []\n",
    "for sentence in preprocessed_list:\n",
    "    # Get the word vectors for each word in the sentence\n",
    "    word_vectors = [word2vec_model.wv[word] for word in sentence]\n",
    "    \n",
    "    # Calculate the sentence vector as the mean of word vectors\n",
    "    sentence_vector = np.mean(word_vectors, axis=0)\n",
    "    \n",
    "    text_vectors.append(sentence_vector)\n",
    "\n",
    "\n",
    "# Convert the list of text vectors into a numpy array containing numerical representations\n",
    "#text_vectors = np.array(text_vectors)\n",
    "text_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input text</th>\n",
       "      <th>input vector</th>\n",
       "      <th>truth text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how often did abraham lincoln cut hi toenail ?</td>\n",
       "      <td>[-0.08713815, 0.21395482, 0.14669764, 0.234587...</td>\n",
       "      <td>{'Every Saturday night': 0, 'Unknown': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when wa abraham lincoln born ?</td>\n",
       "      <td>[-0.08462366, 0.2075645, 0.14377442, 0.2306242...</td>\n",
       "      <td>{'February 12, 1809': 1, 'Unknown': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did barack obama eat for breakfast yester...</td>\n",
       "      <td>[-0.08342842, 0.20322959, 0.13853991, 0.222922...</td>\n",
       "      <td>{'Eggs, potatoes, and toast': 0, 'Unknown': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what doe barack obama like to eat for breakfast ?</td>\n",
       "      <td>[-0.08730293, 0.21139267, 0.14391895, 0.231488...</td>\n",
       "      <td>{'Eggs, potatoes, and toast': 1, 'Unknown': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>who wa the most recent person in the world to ...</td>\n",
       "      <td>[-0.091719195, 0.22258416, 0.15177739, 0.24496...</td>\n",
       "      <td>{'Ariana Grande': 0, 'Unknown': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input text  \\\n",
       "0     how often did abraham lincoln cut hi toenail ?   \n",
       "1                     when wa abraham lincoln born ?   \n",
       "2  what did barack obama eat for breakfast yester...   \n",
       "3  what doe barack obama like to eat for breakfast ?   \n",
       "4  who wa the most recent person in the world to ...   \n",
       "\n",
       "                                        input vector  \\\n",
       "0  [-0.08713815, 0.21395482, 0.14669764, 0.234587...   \n",
       "1  [-0.08462366, 0.2075645, 0.14377442, 0.2306242...   \n",
       "2  [-0.08342842, 0.20322959, 0.13853991, 0.222922...   \n",
       "3  [-0.08730293, 0.21139267, 0.14391895, 0.231488...   \n",
       "4  [-0.091719195, 0.22258416, 0.15177739, 0.24496...   \n",
       "\n",
       "                                       truth text  \n",
       "0       {'Every Saturday night': 0, 'Unknown': 1}  \n",
       "1          {'February 12, 1809': 1, 'Unknown': 0}  \n",
       "2  {'Eggs, potatoes, and toast': 0, 'Unknown': 1}  \n",
       "3  {'Eggs, potatoes, and toast': 1, 'Unknown': 0}  \n",
       "4              {'Ariana Grande': 0, 'Unknown': 1}  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_df = pd.DataFrame({'input text' : preprocessed_list})\n",
    "truth_df = pd.DataFrame({'truth text' : truth_list})\n",
    "\n",
    "#handle as a dictionary to avoid data type issues\n",
    "# combined_data = pd.concat([input_df, truth_df], axis = 1)\n",
    "# combined_data = input_df.join(truth_df)\n",
    "\n",
    "combined_data = {\n",
    "    'input text': preprocessed_list,\n",
    "    'input vector': text_vectors,\n",
    "    'truth text': truth_list\n",
    "}\n",
    "\n",
    "#since we already have the combined data, no need to set a row name using {}\n",
    "combined_df = pd.DataFrame(combined_data)\n",
    "\n",
    "#check\n",
    "combined_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unknown', 'February 12, 1809', 'Unknown', 'Eggs, potatoes, and toast', 'Unknown']\n"
     ]
    }
   ],
   "source": [
    "# extract the correct labels from ground truths\n",
    "correct_labels = []\n",
    "\n",
    "#iterate through the list\n",
    "# convert to dictionary key value pairs to access the dictionary items \n",
    "for item in truth_list:\n",
    "    for label, score in item.items(): \n",
    "        if score == 1:\n",
    "            correct_labels.append(label)\n",
    "\n",
    "print(correct_labels[:5])\n",
    "#this only works if truth_list was a  dictionary\n",
    "#target_label = [label for label, score in truth_list.items() if score == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input text</th>\n",
       "      <th>truth text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how often did abraham lincoln cut hi toenail ?</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when wa abraham lincoln born ?</td>\n",
       "      <td>February 12, 1809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did barack obama eat for breakfast yester...</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what doe barack obama like to eat for breakfast ?</td>\n",
       "      <td>Eggs, potatoes, and toast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input text  \\\n",
       "0     how often did abraham lincoln cut hi toenail ?   \n",
       "1                     when wa abraham lincoln born ?   \n",
       "2  what did barack obama eat for breakfast yester...   \n",
       "3  what doe barack obama like to eat for breakfast ?   \n",
       "\n",
       "                  truth text  \n",
       "0                    Unknown  \n",
       "1          February 12, 1809  \n",
       "2                    Unknown  \n",
       "3  Eggs, potatoes, and toast  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_data = {\n",
    "    'input text': preprocessed_list,\n",
    "    'truth text': correct_labels\n",
    "}\n",
    "\n",
    "reward_mod = pd.DataFrame(reward_data)\n",
    "reward_mod[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Setup \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into train and test sets randomly by percentage weight using the dataframe \n",
    "train_data, test_val_data = train_test_split(combined_df, test_size=0.1, random_state=42)\n",
    "\n",
    "#split test into test and val by percentage weight\n",
    "test_data, val_data = train_test_split(test_val_data, test_size=0.5, random_state=42)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_data, batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Output Layer for post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## Multiple Output Layers ## \n",
    "# Loss Function: Binary Cross-Entropy Loss: binary classification output layer\n",
    "# Mean Squared Error (MSE) or Mean Absolute Error (MAE) : Regression layer to predict a continuous confidence score\n",
    "\n",
    "class CustomOutputLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3_binary = nn.Linear(hidden_size, 1)  # Binary classification output layer\n",
    "        self.fc3_regression = nn.Linear(hidden_size, 1)  # Regression output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        binary_output = torch.sigmoid(self.fc3_binary(x))  # Apply sigmoid for binary classification\n",
    "        regression_output = self.fc3_regression(x)  # Linear activation for regression\n",
    "        return binary_output, regression_output\n",
    "\n",
    "\n",
    "    #training epoch, loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65af070d21cb1b9d17540ebdf37b70f663f41c114c60cbd3317978e563b816d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
